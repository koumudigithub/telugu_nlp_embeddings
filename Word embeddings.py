# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cYav2r5o4AXDccr6UTDG7XLEck3GW7PO
"""

!pip install networkx matplotlib pyvis

from google.colab import files
uploaded = files.upload()

import xml.etree.ElementTree as ET

txt_file_path = 'telugu.output.txt'
output_xml_path = 'telugu.xml'
root = ET.Element("doc", url="http://www.charasala.com/blog/?p=186")

with open(txt_file_path, 'r', encoding='utf-8') as file:
    sentence_elem = None
    for line in file:
        if line.strip().startswith('<') or not line.strip():
            continue

        columns = line.strip().split("\t")
        if len(columns) >= 2:
            word = columns[0]
            lemma = columns[1]
            pos = columns[2] if len(columns) > 2 else None
            suffix = columns[3] if len(columns) > 3 else None
            coarse_pos = columns[4] if len(columns) > 4 else None
            gender = columns[5] if len(columns) > 5 else None
            number = columns[6] if len(columns) > 6 else None
            case = columns[7] if len(columns) > 7 else None

            if sentence_elem is None:
                sentence_elem = ET.SubElement(root, "s")

            word_elem = ET.SubElement(sentence_elem, "word")
            ET.SubElement(word_elem, "lemma").text = lemma
            ET.SubElement(word_elem, "pos").text = pos
            ET.SubElement(word_elem, "suffix").text = suffix
            ET.SubElement(word_elem, "coarse_pos").text = coarse_pos
            ET.SubElement(word_elem, "gender").text = gender
            ET.SubElement(word_elem, "number").text = number
            ET.SubElement(word_elem, "case").text = case

            if "</s>" in line:
                sentence_elem = None

tree = ET.ElementTree(root)
tree.write(output_xml_path, encoding='utf-8', xml_declaration=True)

print(f"XML file written to {output_xml_path}")

import xml.etree.ElementTree as ET
import pandas as pd

xml_file_path = 'telugu.xml'
tree = ET.parse(xml_file_path)
root = tree.getroot()
data = []

for sentence in root.findall('s'):
    for word_elem in sentence.findall('word'):
        word = word_elem.find('lemma').text if word_elem.find('lemma') is not None else None
        lemma = word_elem.find('lemma').text if word_elem.find('lemma') is not None else None
        pos = word_elem.find('pos').text if word_elem.find('pos') is not None else None
        suffix = word_elem.find('suffix').text if word_elem.find('suffix') is not None else None
        coarse_pos = word_elem.find('coarse_pos').text if word_elem.find('coarse_pos') is not None else None
        gender = word_elem.find('gender').text if word_elem.find('gender') is not None else None
        number = word_elem.find('number').text if word_elem.find('number') is not None else None
        case = word_elem.find('case').text if word_elem.find('case') is not None else None

        data.append({
            'word': word,
            'lemma': lemma,
            'pos': pos,
            'suffix': suffix,
            'coarse_pos': coarse_pos,
            'gender': gender,
            'number': number,
            'case': case
        })

df = pd.DataFrame(data)
df.head()

df.style.set_table_styles([
    {'selector': 'thead th', 'props': [('background-color', 'lightblue')]},
    {'selector': 'tbody td', 'props': [('text-align', 'center')]},
])

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

def group_sentences_by_punctuation(df):
    punctuation_marks = ['.', '?', '!']
    sentence_ids = []
    sentence_id = 1

    for _, row in df.iterrows():
        word = row['word']

        if word in punctuation_marks:
            sentence_ids.append(sentence_id)
            sentence_id += 1
        else:
            sentence_ids.append(sentence_id)

    df['sentence_id'] = sentence_ids
    return df

data_with_sentences = group_sentences_by_punctuation(df)
pd.set_option("display.max_rows", None)  # Show all rows
pd.set_option("display.max_columns", None)  # Show all columns
print(data_with_sentences)

adjacency_lists = {}

for sentence_id in df['sentence_id'].unique():
    sentence_df = df[df['sentence_id'] == sentence_id]
    adj_list = {}

    for i, row_i in sentence_df.iterrows():
        pos_i = row_i['pos']

        if pos_i not in adj_list:
            adj_list[pos_i] = []

        for j, row_j in sentence_df.iterrows():
            pos_j = row_j['pos']

            if row_i['pos'] == 'VM' and row_j['pos'] == 'NN':  # Verb (VM) -> Noun (NN)
                adj_list[pos_i].append(pos_j)

            elif row_i['pos'] == 'RB' and row_j['pos'] == 'VM':  # Adverb (RB) -> Verb (VM)
                adj_list[pos_i].append(pos_j)

            elif row_i['pos'] == 'JJ' and row_j['pos'] == 'NN':  # Adjective (JJ) -> Noun (NN)
                adj_list[pos_i].append(pos_j)

            elif row_i['pos'] == 'NN' and row_j['case'] == 'acc':  # Accusative case linking
                adj_list[pos_i].append(pos_j)

    adjacency_lists[sentence_id] = adj_list

for sentence_id, adj_list in adjacency_lists.items():
    print(f"Sentence ID {sentence_id}:")
    for pos, dependencies in adj_list.items():
        print(f"  {pos}: {dependencies}")
    print("\n" + "-"*40 + "\n")

def make_tag(row):
    return f"{row['word']}_{row['gender']}_{row['number']}_{row['case']}"

df['tagged_words'] = df.apply(make_tag, axis=1)

sentences = df.groupby('sentence_id')['tagged_words'].apply(list).tolist()

from gensim.models import Word2Vec

word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, workers=4)
word2vec_model.save("embeddings.model")

for word in word2vec_model.wv.index_to_key:
    print(f"{word}: {word2vec_model.wv[word]}")

print(df.head())

import numpy as np

def get_embedding(word):
    if word in word2vec_model.wv:
        return word2vec_model.wv[word]
    else:
        return np.zeros(word2vec_model.vector_size)

#adding embedding column to df
df["embedding"] = df["tagged_words"].apply(get_embedding)

print(df.head())

import numpy as np

sentence_embeddings = (
    df.groupby("sentence_id")["embedding"]
    .apply(lambda x: np.mean(np.vstack(x), axis=0))  #avg of word vectors of a sentence
    .reset_index()
)

print(sentence_embeddings)